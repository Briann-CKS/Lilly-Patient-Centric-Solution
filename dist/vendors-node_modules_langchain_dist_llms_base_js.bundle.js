"use strict";
/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
(self["webpackChunkapp_1"] = self["webpackChunkapp_1"] || []).push([["vendors-node_modules_langchain_dist_llms_base_js"],{

/***/ "./node_modules/langchain/dist/cache/base.js":
/*!***************************************************!*\
  !*** ./node_modules/langchain/dist/cache/base.js ***!
  \***************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   getCacheKey: () => (/* binding */ getCacheKey)\n/* harmony export */ });\n/* harmony import */ var object_hash__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! object-hash */ \"./node_modules/object-hash/dist/object_hash.js\");\n\n/**\n * This cache key should be consistent across all versions of langchain.\n * It is currently NOT consistent across versions of langchain.\n *\n * A huge benefit of having a remote cache (like redis) is that you can\n * access the cache from different processes/machines. The allows you to\n * seperate concerns and scale horizontally.\n *\n * TODO: Make cache key consistent across versions of langchain.\n */\nconst getCacheKey = (...strings) => object_hash__WEBPACK_IMPORTED_MODULE_0__(strings.join(\"_\"));\n\n\n//# sourceURL=webpack://app-1/./node_modules/langchain/dist/cache/base.js?");

/***/ }),

/***/ "./node_modules/langchain/dist/cache/index.js":
/*!****************************************************!*\
  !*** ./node_modules/langchain/dist/cache/index.js ***!
  \****************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   InMemoryCache: () => (/* binding */ InMemoryCache)\n/* harmony export */ });\n/* harmony import */ var _base_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./base.js */ \"./node_modules/langchain/dist/cache/base.js\");\n/* harmony import */ var _schema_index_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../schema/index.js */ \"./node_modules/langchain/dist/schema/index.js\");\n\n\nconst GLOBAL_MAP = new Map();\nclass InMemoryCache extends _schema_index_js__WEBPACK_IMPORTED_MODULE_1__.BaseCache {\n    constructor(map) {\n        super();\n        Object.defineProperty(this, \"cache\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.cache = map ?? new Map();\n    }\n    lookup(prompt, llmKey) {\n        return Promise.resolve(this.cache.get((0,_base_js__WEBPACK_IMPORTED_MODULE_0__.getCacheKey)(prompt, llmKey)) ?? null);\n    }\n    async update(prompt, llmKey, value) {\n        this.cache.set((0,_base_js__WEBPACK_IMPORTED_MODULE_0__.getCacheKey)(prompt, llmKey), value);\n    }\n    static global() {\n        return new InMemoryCache(GLOBAL_MAP);\n    }\n}\n\n\n//# sourceURL=webpack://app-1/./node_modules/langchain/dist/cache/index.js?");

/***/ }),

/***/ "./node_modules/langchain/dist/llms/base.js":
/*!**************************************************!*\
  !*** ./node_modules/langchain/dist/llms/base.js ***!
  \**************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   BaseLLM: () => (/* binding */ BaseLLM),\n/* harmony export */   LLM: () => (/* binding */ LLM)\n/* harmony export */ });\n/* harmony import */ var _cache_index_js__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../cache/index.js */ \"./node_modules/langchain/dist/cache/index.js\");\n/* harmony import */ var _schema_index_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../schema/index.js */ \"./node_modules/langchain/dist/schema/index.js\");\n/* harmony import */ var _base_language_index_js__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../base_language/index.js */ \"./node_modules/langchain/dist/base_language/index.js\");\n/* harmony import */ var _callbacks_manager_js__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../callbacks/manager.js */ \"./node_modules/langchain/dist/callbacks/manager.js\");\n/* harmony import */ var _memory_base_js__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ../memory/base.js */ \"./node_modules/langchain/dist/memory/base.js\");\n\n\n\n\n\n/**\n * LLM Wrapper. Provides an {@link call} (an {@link generate}) function that takes in a prompt (or prompts) and returns a string.\n */\nclass BaseLLM extends _base_language_index_js__WEBPACK_IMPORTED_MODULE_2__.BaseLanguageModel {\n    constructor({ cache, concurrency, ...rest }) {\n        super(concurrency ? { maxConcurrency: concurrency, ...rest } : rest);\n        Object.defineProperty(this, \"lc_namespace\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: [\"langchain\", \"llms\", this._llmType()]\n        });\n        Object.defineProperty(this, \"cache\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        if (typeof cache === \"object\") {\n            this.cache = cache;\n        }\n        else if (cache) {\n            this.cache = _cache_index_js__WEBPACK_IMPORTED_MODULE_0__.InMemoryCache.global();\n        }\n        else {\n            this.cache = undefined;\n        }\n    }\n    async generatePrompt(promptValues, options, callbacks) {\n        const prompts = promptValues.map((promptValue) => promptValue.toString());\n        return this.generate(prompts, options, callbacks);\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    invocationParams(_options) {\n        return {};\n    }\n    _flattenLLMResult(llmResult) {\n        const llmResults = [];\n        for (let i = 0; i < llmResult.generations.length; i += 1) {\n            const genList = llmResult.generations[i];\n            if (i === 0) {\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput: llmResult.llmOutput,\n                });\n            }\n            else {\n                const llmOutput = llmResult.llmOutput\n                    ? { ...llmResult.llmOutput, tokenUsage: {} }\n                    : undefined;\n                llmResults.push({\n                    generations: [genList],\n                    llmOutput,\n                });\n            }\n        }\n        return llmResults;\n    }\n    /** @ignore */\n    async _generateUncached(prompts, parsedOptions, handledOptions) {\n        const callbackManager_ = await _callbacks_manager_js__WEBPACK_IMPORTED_MODULE_3__.CallbackManager.configure(handledOptions.callbacks, this.callbacks, handledOptions.tags, this.tags, handledOptions.metadata, this.metadata, { verbose: this.verbose });\n        const extra = {\n            options: parsedOptions,\n            invocation_params: this?.invocationParams(parsedOptions),\n        };\n        const runManagers = await callbackManager_?.handleLLMStart(this.toJSON(), prompts, undefined, undefined, extra);\n        let output;\n        try {\n            output = await this._generate(prompts, parsedOptions, runManagers?.[0]);\n        }\n        catch (err) {\n            await Promise.all((runManagers ?? []).map((runManager) => runManager?.handleLLMError(err)));\n            throw err;\n        }\n        const flattenedOutputs = this._flattenLLMResult(output);\n        await Promise.all((runManagers ?? []).map((runManager, i) => runManager?.handleLLMEnd(flattenedOutputs[i])));\n        const runIds = runManagers?.map((manager) => manager.runId) || undefined;\n        // This defines RUN_KEY as a non-enumerable property on the output object\n        // so that it is not serialized when the output is stringified, and so that\n        // it isnt included when listing the keys of the output object.\n        Object.defineProperty(output, _schema_index_js__WEBPACK_IMPORTED_MODULE_1__.RUN_KEY, {\n            value: runIds ? { runIds } : undefined,\n            configurable: true,\n        });\n        return output;\n    }\n    /**\n     * Run the LLM on the given propmts an input, handling caching.\n     */\n    async generate(prompts, options, callbacks) {\n        if (!Array.isArray(prompts)) {\n            throw new Error(\"Argument 'prompts' is expected to be a string[]\");\n        }\n        let parsedOptions;\n        if (Array.isArray(options)) {\n            parsedOptions = { stop: options };\n        }\n        else if (options?.timeout && !options.signal) {\n            parsedOptions = {\n                ...options,\n                signal: AbortSignal.timeout(options.timeout),\n            };\n        }\n        else {\n            parsedOptions = options ?? {};\n        }\n        const handledOptions = {\n            tags: parsedOptions.tags,\n            metadata: parsedOptions.metadata,\n            callbacks: parsedOptions.callbacks ?? callbacks,\n        };\n        delete parsedOptions.tags;\n        delete parsedOptions.metadata;\n        delete parsedOptions.callbacks;\n        if (!this.cache) {\n            return this._generateUncached(prompts, parsedOptions, handledOptions);\n        }\n        const { cache } = this;\n        const params = this.serialize();\n        params.stop = parsedOptions.stop ?? params.stop;\n        const llmStringKey = `${Object.entries(params).sort()}`;\n        const missingPromptIndices = [];\n        const generations = await Promise.all(prompts.map(async (prompt, index) => {\n            const result = await cache.lookup(prompt, llmStringKey);\n            if (!result) {\n                missingPromptIndices.push(index);\n            }\n            return result;\n        }));\n        let llmOutput = {};\n        if (missingPromptIndices.length > 0) {\n            const results = await this._generateUncached(missingPromptIndices.map((i) => prompts[i]), parsedOptions, handledOptions);\n            await Promise.all(results.generations.map(async (generation, index) => {\n                const promptIndex = missingPromptIndices[index];\n                generations[promptIndex] = generation;\n                return cache.update(prompts[promptIndex], llmStringKey, generation);\n            }));\n            llmOutput = results.llmOutput ?? {};\n        }\n        return { generations, llmOutput };\n    }\n    /**\n     * Convenience wrapper for {@link generate} that takes in a single string prompt and returns a single string output.\n     */\n    async call(prompt, options, callbacks) {\n        const { generations } = await this.generate([prompt], options ?? {}, callbacks);\n        return generations[0][0].text;\n    }\n    async predict(text, options, callbacks) {\n        return this.call(text, options, callbacks);\n    }\n    async predictMessages(messages, options, callbacks) {\n        const text = (0,_memory_base_js__WEBPACK_IMPORTED_MODULE_4__.getBufferString)(messages);\n        const prediction = await this.call(text, options, callbacks);\n        return new _schema_index_js__WEBPACK_IMPORTED_MODULE_1__.AIMessage(prediction);\n    }\n    /**\n     * Get the identifying parameters of the LLM.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _identifyingParams() {\n        return {};\n    }\n    /**\n     * Return a json-like object representing this LLM.\n     */\n    serialize() {\n        return {\n            ...this._identifyingParams(),\n            _type: this._llmType(),\n            _model: this._modelType(),\n        };\n    }\n    _modelType() {\n        return \"base_llm\";\n    }\n    /**\n     * Load an LLM from a json-like object describing it.\n     */\n    static async deserialize(data) {\n        const { _type, _model, ...rest } = data;\n        if (_model && _model !== \"base_llm\") {\n            throw new Error(`Cannot load LLM with model ${_model}`);\n        }\n        const Cls = {\n            openai: (await Promise.resolve(/*! import() */).then(__webpack_require__.bind(__webpack_require__, /*! ./openai.js */ \"./node_modules/langchain/dist/llms/openai.js\"))).OpenAI,\n        }[_type];\n        if (Cls === undefined) {\n            throw new Error(`Cannot load  LLM with type ${_type}`);\n        }\n        return new Cls(rest);\n    }\n}\n/**\n * LLM class that provides a simpler interface to subclass than {@link BaseLLM}.\n *\n * Requires only implementing a simpler {@link _call} method instead of {@link _generate}.\n *\n * @augments BaseLLM\n */\nclass LLM extends BaseLLM {\n    async _generate(prompts, options, runManager) {\n        const generations = await Promise.all(prompts.map((prompt, promptIndex) => this._call(prompt, { ...options, promptIndex }, runManager).then((text) => [{ text }])));\n        return { generations };\n    }\n}\n\n\n//# sourceURL=webpack://app-1/./node_modules/langchain/dist/llms/base.js?");

/***/ }),

/***/ "./node_modules/langchain/dist/llms/openai-chat.js":
/*!*********************************************************!*\
  !*** ./node_modules/langchain/dist/llms/openai-chat.js ***!
  \*********************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   OpenAIChat: () => (/* binding */ OpenAIChat),\n/* harmony export */   PromptLayerOpenAIChat: () => (/* binding */ PromptLayerOpenAIChat)\n/* harmony export */ });\n/* harmony import */ var openai__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! openai */ \"./node_modules/openai/dist/index.js\");\n/* harmony import */ var _util_env_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../util/env.js */ \"./node_modules/langchain/dist/util/env.js\");\n/* harmony import */ var _util_axios_fetch_adapter_js__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../util/axios-fetch-adapter.js */ \"./node_modules/langchain/dist/util/axios-fetch-adapter.js\");\n/* harmony import */ var _base_js__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./base.js */ \"./node_modules/langchain/dist/llms/base.js\");\n/* harmony import */ var _util_prompt_layer_js__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ../util/prompt-layer.js */ \"./node_modules/langchain/dist/util/prompt-layer.js\");\n/* harmony import */ var _util_azure_js__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! ../util/azure.js */ \"./node_modules/langchain/dist/util/azure.js\");\n\n\n\n\n\n\n/**\n * Wrapper around OpenAI large language models that use the Chat endpoint.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/chat/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n *\n * @augments BaseLLM\n * @augments OpenAIInput\n * @augments AzureOpenAIChatInput\n */\nclass OpenAIChat extends _base_js__WEBPACK_IMPORTED_MODULE_3__.LLM {\n    get callKeys() {\n        return [\n            ...super.callKeys,\n            \"options\",\n            \"promptIndex\",\n        ];\n    }\n    get lc_secrets() {\n        return {\n            openAIApiKey: \"OPENAI_API_KEY\",\n            azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n        };\n    }\n    get lc_aliases() {\n        return {\n            modelName: \"model\",\n            openAIApiKey: \"openai_api_key\",\n            azureOpenAIApiVersion: \"azure_openai_api_version\",\n            azureOpenAIApiKey: \"azure_openai_api_key\",\n            azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n            azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\",\n        };\n    }\n    constructor(fields, \n    /** @deprecated */\n    configuration) {\n        super(fields ?? {});\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"gpt-3.5-turbo\"\n        });\n        Object.defineProperty(this, \"prefixMessages\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"openAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIBasePath\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.openAIApiKey =\n            fields?.openAIApiKey ?? (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"OPENAI_API_KEY\");\n        this.azureOpenAIApiKey =\n            fields?.azureOpenAIApiKey ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_KEY\");\n        if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n            throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n        }\n        this.azureOpenAIApiInstanceName =\n            fields?.azureOpenAIApiInstanceName ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n        this.azureOpenAIApiDeploymentName =\n            (fields?.azureOpenAIApiCompletionsDeploymentName ||\n                fields?.azureOpenAIApiDeploymentName) ??\n                ((0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\") ||\n                    (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n        this.azureOpenAIApiVersion =\n            fields?.azureOpenAIApiVersion ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_VERSION\");\n        this.azureOpenAIBasePath =\n            fields?.azureOpenAIBasePath ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_BASE_PATH\");\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.prefixMessages = fields?.prefixMessages ?? this.prefixMessages;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.n = fields?.n ?? this.n;\n        this.logitBias = fields?.logitBias;\n        this.maxTokens = fields?.maxTokens;\n        this.stop = fields?.stop;\n        this.streaming = fields?.streaming ?? false;\n        if (this.n > 1) {\n            throw new Error(\"Cannot use n > 1 in OpenAIChat LLM. Use ChatOpenAI Chat Model instead.\");\n        }\n        if (this.azureOpenAIApiKey) {\n            if (!this.azureOpenAIApiInstanceName) {\n                throw new Error(\"Azure OpenAI API instance name not found\");\n            }\n            if (!this.azureOpenAIApiDeploymentName) {\n                throw new Error(\"Azure OpenAI API deployment name not found\");\n            }\n            if (!this.azureOpenAIApiVersion) {\n                throw new Error(\"Azure OpenAI API version not found\");\n            }\n        }\n        this.clientConfig = {\n            apiKey: this.openAIApiKey,\n            ...configuration,\n            ...fields?.configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(options) {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            n: this.n,\n            logit_bias: this.logitBias,\n            max_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n            stop: options?.stop ?? this.stop,\n            stream: this.streaming,\n            ...this.modelKwargs,\n        };\n    }\n    /** @ignore */\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    formatMessages(prompt) {\n        const message = {\n            role: \"user\",\n            content: prompt,\n        };\n        return this.prefixMessages ? [...this.prefixMessages, message] : [message];\n    }\n    /** @ignore */\n    async _call(prompt, options, runManager) {\n        const params = this.invocationParams(options);\n        const data = params.stream\n            ? await new Promise((resolve, reject) => {\n                let response;\n                let rejected = false;\n                let resolved = false;\n                this.completionWithRetry({\n                    ...params,\n                    messages: this.formatMessages(prompt),\n                }, {\n                    signal: options.signal,\n                    ...options.options,\n                    adapter: _util_axios_fetch_adapter_js__WEBPACK_IMPORTED_MODULE_2__[\"default\"],\n                    responseType: \"stream\",\n                    onmessage: (event) => {\n                        if (event.data?.trim?.() === \"[DONE]\") {\n                            if (resolved || rejected) {\n                                return;\n                            }\n                            resolved = true;\n                            resolve(response);\n                        }\n                        else {\n                            const data = JSON.parse(event.data);\n                            if (data?.error) {\n                                if (rejected) {\n                                    return;\n                                }\n                                rejected = true;\n                                reject(data.error);\n                                return;\n                            }\n                            const message = data;\n                            // on the first message set the response properties\n                            if (!response) {\n                                response = {\n                                    id: message.id,\n                                    object: message.object,\n                                    created: message.created,\n                                    model: message.model,\n                                    choices: [],\n                                };\n                            }\n                            // on all messages, update choice\n                            for (const part of message.choices) {\n                                if (part != null) {\n                                    let choice = response.choices.find((c) => c.index === part.index);\n                                    if (!choice) {\n                                        choice = {\n                                            index: part.index,\n                                            finish_reason: part.finish_reason ?? undefined,\n                                        };\n                                        response.choices.push(choice);\n                                    }\n                                    if (!choice.message) {\n                                        choice.message = {\n                                            role: part.delta\n                                                ?.role,\n                                            content: part.delta?.content ?? \"\",\n                                        };\n                                    }\n                                    choice.message.content += part.delta?.content ?? \"\";\n                                    // eslint-disable-next-line no-void\n                                    void runManager?.handleLLMNewToken(part.delta?.content ?? \"\", {\n                                        prompt: options.promptIndex ?? 0,\n                                        completion: part.index,\n                                    });\n                                }\n                            }\n                            // when all messages are finished, resolve\n                            if (!resolved &&\n                                !rejected &&\n                                message.choices.every((c) => c.finish_reason != null)) {\n                                resolved = true;\n                                resolve(response);\n                            }\n                        }\n                    },\n                }).catch((error) => {\n                    if (!rejected) {\n                        rejected = true;\n                        reject(error);\n                    }\n                });\n            })\n            : await this.completionWithRetry({\n                ...params,\n                messages: this.formatMessages(prompt),\n            }, {\n                signal: options.signal,\n                ...options.options,\n            });\n        return data.choices[0].message?.content ?? \"\";\n    }\n    /** @ignore */\n    async completionWithRetry(request, options) {\n        if (!this.client) {\n            const openAIEndpointConfig = {\n                azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n                azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n                azureOpenAIApiKey: this.azureOpenAIApiKey,\n                azureOpenAIBasePath: this.azureOpenAIBasePath,\n                basePath: this.clientConfig.basePath,\n            };\n            const endpoint = (0,_util_azure_js__WEBPACK_IMPORTED_MODULE_5__.getEndpoint)(openAIEndpointConfig);\n            const clientConfig = new openai__WEBPACK_IMPORTED_MODULE_0__.Configuration({\n                ...this.clientConfig,\n                basePath: endpoint,\n                baseOptions: {\n                    timeout: this.timeout,\n                    ...this.clientConfig.baseOptions,\n                },\n            });\n            this.client = new openai__WEBPACK_IMPORTED_MODULE_0__.OpenAIApi(clientConfig);\n        }\n        const axiosOptions = {\n            adapter: (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.isNode)() ? undefined : _util_axios_fetch_adapter_js__WEBPACK_IMPORTED_MODULE_2__[\"default\"],\n            ...this.clientConfig.baseOptions,\n            ...options,\n        };\n        if (this.azureOpenAIApiKey) {\n            axiosOptions.headers = {\n                \"api-key\": this.azureOpenAIApiKey,\n                ...axiosOptions.headers,\n            };\n            axiosOptions.params = {\n                \"api-version\": this.azureOpenAIApiVersion,\n                ...axiosOptions.params,\n            };\n        }\n        return this.caller\n            .call(this.client.createChatCompletion.bind(this.client), request, axiosOptions)\n            .then((res) => res.data);\n    }\n    _llmType() {\n        return \"openai\";\n    }\n}\n/**\n * PromptLayer wrapper to OpenAIChat\n */\nclass PromptLayerOpenAIChat extends OpenAIChat {\n    get lc_secrets() {\n        return {\n            promptLayerApiKey: \"PROMPTLAYER_API_KEY\",\n        };\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"promptLayerApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"plTags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"returnPromptLayerId\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.plTags = fields?.plTags ?? [];\n        this.returnPromptLayerId = fields?.returnPromptLayerId ?? false;\n        this.promptLayerApiKey =\n            fields?.promptLayerApiKey ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"PROMPTLAYER_API_KEY\");\n        if (!this.promptLayerApiKey) {\n            throw new Error(\"Missing PromptLayer API key\");\n        }\n    }\n    async completionWithRetry(request, options) {\n        if (request.stream) {\n            return super.completionWithRetry(request, options);\n        }\n        const response = await super.completionWithRetry(request);\n        return response;\n    }\n    async _generate(prompts, options, runManager) {\n        let choice;\n        const generations = await Promise.all(prompts.map(async (prompt) => {\n            const requestStartTime = Date.now();\n            const text = await this._call(prompt, options, runManager);\n            const requestEndTime = Date.now();\n            choice = [{ text }];\n            const parsedResp = {\n                text,\n            };\n            const promptLayerRespBody = await (0,_util_prompt_layer_js__WEBPACK_IMPORTED_MODULE_4__.promptLayerTrackRequest)(this.caller, \"langchain.PromptLayerOpenAIChat\", [prompt], this._identifyingParams(), this.plTags, parsedResp, requestStartTime, requestEndTime, this.promptLayerApiKey);\n            if (this.returnPromptLayerId === true &&\n                promptLayerRespBody.success === true) {\n                choice[0].generationInfo = {\n                    promptLayerRequestId: promptLayerRespBody.request_id,\n                };\n            }\n            return choice;\n        }));\n        return { generations };\n    }\n}\n\n\n//# sourceURL=webpack://app-1/./node_modules/langchain/dist/llms/openai-chat.js?");

/***/ }),

/***/ "./node_modules/langchain/dist/llms/openai.js":
/*!****************************************************!*\
  !*** ./node_modules/langchain/dist/llms/openai.js ***!
  \****************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   OpenAI: () => (/* binding */ OpenAI),\n/* harmony export */   OpenAIChat: () => (/* reexport safe */ _openai_chat_js__WEBPACK_IMPORTED_MODULE_6__.OpenAIChat),\n/* harmony export */   PromptLayerOpenAI: () => (/* binding */ PromptLayerOpenAI),\n/* harmony export */   PromptLayerOpenAIChat: () => (/* reexport safe */ _openai_chat_js__WEBPACK_IMPORTED_MODULE_6__.PromptLayerOpenAIChat)\n/* harmony export */ });\n/* harmony import */ var openai__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! openai */ \"./node_modules/openai/dist/index.js\");\n/* harmony import */ var _util_env_js__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../util/env.js */ \"./node_modules/langchain/dist/util/env.js\");\n/* harmony import */ var _util_axios_fetch_adapter_js__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../util/axios-fetch-adapter.js */ \"./node_modules/langchain/dist/util/axios-fetch-adapter.js\");\n/* harmony import */ var _util_chunk_js__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../util/chunk.js */ \"./node_modules/langchain/dist/util/chunk.js\");\n/* harmony import */ var _base_js__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ./base.js */ \"./node_modules/langchain/dist/llms/base.js\");\n/* harmony import */ var _base_language_count_tokens_js__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! ../base_language/count_tokens.js */ \"./node_modules/langchain/dist/base_language/count_tokens.js\");\n/* harmony import */ var _openai_chat_js__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! ./openai-chat.js */ \"./node_modules/langchain/dist/llms/openai-chat.js\");\n/* harmony import */ var _util_prompt_layer_js__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ../util/prompt-layer.js */ \"./node_modules/langchain/dist/util/prompt-layer.js\");\n/* harmony import */ var _util_azure_js__WEBPACK_IMPORTED_MODULE_8__ = __webpack_require__(/*! ../util/azure.js */ \"./node_modules/langchain/dist/util/azure.js\");\n\n\n\n\n\n\n\n\n\n/**\n * Wrapper around OpenAI large language models.\n *\n * To use you should have the `openai` package installed, with the\n * `OPENAI_API_KEY` environment variable set.\n *\n * To use with Azure you should have the `openai` package installed, with the\n * `AZURE_OPENAI_API_KEY`,\n * `AZURE_OPENAI_API_INSTANCE_NAME`,\n * `AZURE_OPENAI_API_DEPLOYMENT_NAME`\n * and `AZURE_OPENAI_API_VERSION` environment variable set.\n *\n * @remarks\n * Any parameters that are valid to be passed to {@link\n * https://platform.openai.com/docs/api-reference/completions/create |\n * `openai.createCompletion`} can be passed through {@link modelKwargs}, even\n * if not explicitly available on this class.\n */\nclass OpenAI extends _base_js__WEBPACK_IMPORTED_MODULE_4__.BaseLLM {\n    get callKeys() {\n        return [...super.callKeys, \"options\"];\n    }\n    get lc_secrets() {\n        return {\n            openAIApiKey: \"OPENAI_API_KEY\",\n            azureOpenAIApiKey: \"AZURE_OPENAI_API_KEY\",\n        };\n    }\n    get lc_aliases() {\n        return {\n            modelName: \"model\",\n            openAIApiKey: \"openai_api_key\",\n            azureOpenAIApiVersion: \"azure_openai_api_version\",\n            azureOpenAIApiKey: \"azure_openai_api_key\",\n            azureOpenAIApiInstanceName: \"azure_openai_api_instance_name\",\n            azureOpenAIApiDeploymentName: \"azure_openai_api_deployment_name\",\n        };\n    }\n    constructor(fields, \n    /** @deprecated */\n    configuration) {\n        if (fields?.modelName?.startsWith(\"gpt-3.5-turbo\") ||\n            fields?.modelName?.startsWith(\"gpt-4\") ||\n            fields?.modelName?.startsWith(\"gpt-4-32k\")) {\n            // eslint-disable-next-line no-constructor-return, @typescript-eslint/no-explicit-any\n            return new _openai_chat_js__WEBPACK_IMPORTED_MODULE_6__.OpenAIChat(fields, configuration);\n        }\n        super(fields ?? {});\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: true\n        });\n        Object.defineProperty(this, \"temperature\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0.7\n        });\n        Object.defineProperty(this, \"maxTokens\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 256\n        });\n        Object.defineProperty(this, \"topP\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"frequencyPenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"presencePenalty\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 0\n        });\n        Object.defineProperty(this, \"n\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 1\n        });\n        Object.defineProperty(this, \"bestOf\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"logitBias\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"modelName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: \"text-davinci-003\"\n        });\n        Object.defineProperty(this, \"modelKwargs\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"batchSize\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: 20\n        });\n        Object.defineProperty(this, \"timeout\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"stop\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"streaming\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"openAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiVersion\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiInstanceName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIApiDeploymentName\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"azureOpenAIBasePath\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"client\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"clientConfig\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.openAIApiKey =\n            fields?.openAIApiKey ?? (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"OPENAI_API_KEY\");\n        this.azureOpenAIApiKey =\n            fields?.azureOpenAIApiKey ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_KEY\");\n        if (!this.azureOpenAIApiKey && !this.openAIApiKey) {\n            throw new Error(\"OpenAI or Azure OpenAI API key not found\");\n        }\n        this.azureOpenAIApiInstanceName =\n            fields?.azureOpenAIApiInstanceName ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_INSTANCE_NAME\");\n        this.azureOpenAIApiDeploymentName =\n            (fields?.azureOpenAIApiCompletionsDeploymentName ||\n                fields?.azureOpenAIApiDeploymentName) ??\n                ((0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\") ||\n                    (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_DEPLOYMENT_NAME\"));\n        this.azureOpenAIApiVersion =\n            fields?.azureOpenAIApiVersion ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_API_VERSION\");\n        this.azureOpenAIBasePath =\n            fields?.azureOpenAIBasePath ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"AZURE_OPENAI_BASE_PATH\");\n        this.modelName = fields?.modelName ?? this.modelName;\n        this.modelKwargs = fields?.modelKwargs ?? {};\n        this.batchSize = fields?.batchSize ?? this.batchSize;\n        this.timeout = fields?.timeout;\n        this.temperature = fields?.temperature ?? this.temperature;\n        this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n        this.topP = fields?.topP ?? this.topP;\n        this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n        this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n        this.n = fields?.n ?? this.n;\n        this.bestOf = fields?.bestOf ?? this.bestOf;\n        this.logitBias = fields?.logitBias;\n        this.stop = fields?.stop;\n        this.streaming = fields?.streaming ?? false;\n        if (this.streaming && this.bestOf && this.bestOf > 1) {\n            throw new Error(\"Cannot stream results when bestOf > 1\");\n        }\n        if (this.azureOpenAIApiKey) {\n            if (!this.azureOpenAIApiInstanceName) {\n                throw new Error(\"Azure OpenAI API instance name not found\");\n            }\n            if (!this.azureOpenAIApiDeploymentName) {\n                throw new Error(\"Azure OpenAI API deployment name not found\");\n            }\n            if (!this.azureOpenAIApiVersion) {\n                throw new Error(\"Azure OpenAI API version not found\");\n            }\n        }\n        this.clientConfig = {\n            apiKey: this.openAIApiKey,\n            ...configuration,\n            ...fields?.configuration,\n        };\n    }\n    /**\n     * Get the parameters used to invoke the model\n     */\n    invocationParams(options) {\n        return {\n            model: this.modelName,\n            temperature: this.temperature,\n            max_tokens: this.maxTokens,\n            top_p: this.topP,\n            frequency_penalty: this.frequencyPenalty,\n            presence_penalty: this.presencePenalty,\n            n: this.n,\n            best_of: this.bestOf,\n            logit_bias: this.logitBias,\n            stop: options?.stop ?? this.stop,\n            stream: this.streaming,\n            ...this.modelKwargs,\n        };\n    }\n    _identifyingParams() {\n        return {\n            model_name: this.modelName,\n            ...this.invocationParams(),\n            ...this.clientConfig,\n        };\n    }\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams() {\n        return this._identifyingParams();\n    }\n    /**\n     * Call out to OpenAI's endpoint with k unique prompts\n     *\n     * @param [prompts] - The prompts to pass into the model.\n     * @param [options] - Optional list of stop words to use when generating.\n     * @param [runManager] - Optional callback manager to use when generating.\n     *\n     * @returns The full LLM output.\n     *\n     * @example\n     * ```ts\n     * import { OpenAI } from \"langchain/llms/openai\";\n     * const openai = new OpenAI();\n     * const response = await openai.generate([\"Tell me a joke.\"]);\n     * ```\n     */\n    async _generate(prompts, options, runManager) {\n        const subPrompts = (0,_util_chunk_js__WEBPACK_IMPORTED_MODULE_3__.chunkArray)(prompts, this.batchSize);\n        const choices = [];\n        const tokenUsage = {};\n        const params = this.invocationParams(options);\n        if (params.max_tokens === -1) {\n            if (prompts.length !== 1) {\n                throw new Error(\"max_tokens set to -1 not supported for multiple inputs\");\n            }\n            params.max_tokens = await (0,_base_language_count_tokens_js__WEBPACK_IMPORTED_MODULE_5__.calculateMaxTokens)({\n                prompt: prompts[0],\n                // Cast here to allow for other models that may not fit the union\n                modelName: this.modelName,\n            });\n        }\n        for (let i = 0; i < subPrompts.length; i += 1) {\n            const data = params.stream\n                ? await new Promise((resolve, reject) => {\n                    const choices = [];\n                    let response;\n                    let rejected = false;\n                    let resolved = false;\n                    this.completionWithRetry({\n                        ...params,\n                        prompt: subPrompts[i],\n                    }, {\n                        signal: options.signal,\n                        ...options.options,\n                        adapter: _util_axios_fetch_adapter_js__WEBPACK_IMPORTED_MODULE_2__[\"default\"],\n                        responseType: \"stream\",\n                        onmessage: (event) => {\n                            if (event.data?.trim?.() === \"[DONE]\") {\n                                if (resolved || rejected) {\n                                    return;\n                                }\n                                resolved = true;\n                                resolve({\n                                    ...response,\n                                    choices,\n                                });\n                            }\n                            else {\n                                const data = JSON.parse(event.data);\n                                if (data?.error) {\n                                    if (rejected) {\n                                        return;\n                                    }\n                                    rejected = true;\n                                    reject(data.error);\n                                    return;\n                                }\n                                const message = data;\n                                // on the first message set the response properties\n                                if (!response) {\n                                    response = {\n                                        id: message.id,\n                                        object: message.object,\n                                        created: message.created,\n                                        model: message.model,\n                                    };\n                                }\n                                // on all messages, update choice\n                                for (const part of message.choices) {\n                                    if (part != null && part.index != null) {\n                                        if (!choices[part.index])\n                                            choices[part.index] = {};\n                                        const choice = choices[part.index];\n                                        choice.text = (choice.text ?? \"\") + (part.text ?? \"\");\n                                        choice.finish_reason = part.finish_reason;\n                                        choice.logprobs = part.logprobs;\n                                        // eslint-disable-next-line no-void\n                                        void runManager?.handleLLMNewToken(part.text ?? \"\", {\n                                            prompt: Math.floor(part.index / this.n),\n                                            completion: part.index % this.n,\n                                        });\n                                    }\n                                }\n                                // when all messages are finished, resolve\n                                if (!resolved &&\n                                    !rejected &&\n                                    choices.every((c) => c.finish_reason != null)) {\n                                    resolved = true;\n                                    resolve({\n                                        ...response,\n                                        choices,\n                                    });\n                                }\n                            }\n                        },\n                    }).catch((error) => {\n                        if (!rejected) {\n                            rejected = true;\n                            reject(error);\n                        }\n                    });\n                })\n                : await this.completionWithRetry({\n                    ...params,\n                    prompt: subPrompts[i],\n                }, {\n                    signal: options.signal,\n                    ...options.options,\n                });\n            choices.push(...data.choices);\n            const { completion_tokens: completionTokens, prompt_tokens: promptTokens, total_tokens: totalTokens, } = data.usage ?? {};\n            if (completionTokens) {\n                tokenUsage.completionTokens =\n                    (tokenUsage.completionTokens ?? 0) + completionTokens;\n            }\n            if (promptTokens) {\n                tokenUsage.promptTokens = (tokenUsage.promptTokens ?? 0) + promptTokens;\n            }\n            if (totalTokens) {\n                tokenUsage.totalTokens = (tokenUsage.totalTokens ?? 0) + totalTokens;\n            }\n        }\n        const generations = (0,_util_chunk_js__WEBPACK_IMPORTED_MODULE_3__.chunkArray)(choices, this.n).map((promptChoices) => promptChoices.map((choice) => ({\n            text: choice.text ?? \"\",\n            generationInfo: {\n                finishReason: choice.finish_reason,\n                logprobs: choice.logprobs,\n            },\n        })));\n        return {\n            generations,\n            llmOutput: { tokenUsage },\n        };\n    }\n    /** @ignore */\n    async completionWithRetry(request, options) {\n        if (!this.client) {\n            const openAIEndpointConfig = {\n                azureOpenAIApiDeploymentName: this.azureOpenAIApiDeploymentName,\n                azureOpenAIApiInstanceName: this.azureOpenAIApiInstanceName,\n                azureOpenAIApiKey: this.azureOpenAIApiKey,\n                azureOpenAIBasePath: this.azureOpenAIBasePath,\n                basePath: this.clientConfig.basePath,\n            };\n            const endpoint = (0,_util_azure_js__WEBPACK_IMPORTED_MODULE_8__.getEndpoint)(openAIEndpointConfig);\n            const clientConfig = new openai__WEBPACK_IMPORTED_MODULE_0__.Configuration({\n                ...this.clientConfig,\n                basePath: endpoint,\n                baseOptions: {\n                    timeout: this.timeout,\n                    ...this.clientConfig.baseOptions,\n                },\n            });\n            this.client = new openai__WEBPACK_IMPORTED_MODULE_0__.OpenAIApi(clientConfig);\n        }\n        const axiosOptions = {\n            adapter: (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.isNode)() ? undefined : _util_axios_fetch_adapter_js__WEBPACK_IMPORTED_MODULE_2__[\"default\"],\n            ...this.clientConfig.baseOptions,\n            ...options,\n        };\n        if (this.azureOpenAIApiKey) {\n            axiosOptions.headers = {\n                \"api-key\": this.azureOpenAIApiKey,\n                ...axiosOptions.headers,\n            };\n            axiosOptions.params = {\n                \"api-version\": this.azureOpenAIApiVersion,\n                ...axiosOptions.params,\n            };\n        }\n        return this.caller\n            .call(this.client.createCompletion.bind(this.client), request, axiosOptions)\n            .then((res) => res.data);\n    }\n    _llmType() {\n        return \"openai\";\n    }\n}\n/**\n * PromptLayer wrapper to OpenAI\n * @augments OpenAI\n */\nclass PromptLayerOpenAI extends OpenAI {\n    get lc_secrets() {\n        return {\n            promptLayerApiKey: \"PROMPTLAYER_API_KEY\",\n        };\n    }\n    constructor(fields) {\n        super(fields);\n        Object.defineProperty(this, \"lc_serializable\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: false\n        });\n        Object.defineProperty(this, \"promptLayerApiKey\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"plTags\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        Object.defineProperty(this, \"returnPromptLayerId\", {\n            enumerable: true,\n            configurable: true,\n            writable: true,\n            value: void 0\n        });\n        this.plTags = fields?.plTags ?? [];\n        this.promptLayerApiKey =\n            fields?.promptLayerApiKey ??\n                (0,_util_env_js__WEBPACK_IMPORTED_MODULE_1__.getEnvironmentVariable)(\"PROMPTLAYER_API_KEY\");\n        this.returnPromptLayerId = fields?.returnPromptLayerId;\n        if (!this.promptLayerApiKey) {\n            throw new Error(\"Missing PromptLayer API key\");\n        }\n    }\n    async completionWithRetry(request, options) {\n        if (request.stream) {\n            return super.completionWithRetry(request, options);\n        }\n        const response = await super.completionWithRetry(request);\n        return response;\n    }\n    async _generate(prompts, options, runManager) {\n        const requestStartTime = Date.now();\n        const generations = await super._generate(prompts, options, runManager);\n        for (let i = 0; i < generations.generations.length; i += 1) {\n            const requestEndTime = Date.now();\n            const parsedResp = {\n                text: generations.generations[i][0].text,\n                llm_output: generations.llmOutput,\n            };\n            const promptLayerRespBody = await (0,_util_prompt_layer_js__WEBPACK_IMPORTED_MODULE_7__.promptLayerTrackRequest)(this.caller, \"langchain.PromptLayerOpenAI\", [prompts[i]], this._identifyingParams(), this.plTags, parsedResp, requestStartTime, requestEndTime, this.promptLayerApiKey);\n            let promptLayerRequestId;\n            if (this.returnPromptLayerId === true) {\n                if (promptLayerRespBody && promptLayerRespBody.success === true) {\n                    promptLayerRequestId = promptLayerRespBody.request_id;\n                }\n                generations.generations[i][0].generationInfo = {\n                    ...generations.generations[i][0].generationInfo,\n                    promptLayerRequestId,\n                };\n            }\n        }\n        return generations;\n    }\n}\n\n\n\n//# sourceURL=webpack://app-1/./node_modules/langchain/dist/llms/openai.js?");

/***/ }),

/***/ "./node_modules/langchain/dist/util/chunk.js":
/*!***************************************************!*\
  !*** ./node_modules/langchain/dist/util/chunk.js ***!
  \***************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   chunkArray: () => (/* binding */ chunkArray)\n/* harmony export */ });\nconst chunkArray = (arr, chunkSize) => arr.reduce((chunks, elem, index) => {\n    const chunkIndex = Math.floor(index / chunkSize);\n    const chunk = chunks[chunkIndex] || [];\n    // eslint-disable-next-line no-param-reassign\n    chunks[chunkIndex] = chunk.concat([elem]);\n    return chunks;\n}, []);\n\n\n//# sourceURL=webpack://app-1/./node_modules/langchain/dist/util/chunk.js?");

/***/ })

}]);